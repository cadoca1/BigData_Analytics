{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicios SesiÃ³n 19 TD Real Projects 30.01.2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Jq9d0x1OTh2N"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9d0x1OTh2N"
      },
      "source": [
        "# Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_DQBVj_KNvL"
      },
      "source": [
        "Installing Spark and Apache Kafka Library in VM\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbGSM3_NM-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ab0769-535c-48a1-9a2a-7651f61ecc7d"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark==1.3.0\n",
        "!pip install py4j==0.10.9\n",
        "\n",
        "# For plotting\n",
        "!pip install folium\n",
        "!pip install plotly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 15.2MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20kB 20.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 40kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 71kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 81kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 92kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 102kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 112kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 122kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 133kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 143kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 153kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 163kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 174kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 184kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 194kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 7.1MB/s \n",
            "\u001b[?25hInstalling collected packages: py4j\n",
            "Successfully installed py4j-0.10.9\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from folium) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from folium) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from folium) (2.11.2)\n",
            "Requirement already satisfied: branca>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from folium) (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from folium) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->folium) (1.1.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (4.4.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5sHazLNzqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012b3f1c-7f3a-4936-c6f7-9e62703d6ac3"
      },
      "source": [
        "!ls /content"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spark-3.0.1-bin-hadoop2.7\tspark-3.0.1-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_HtvSAj4sI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80a3cd2-bec0-471e-c02d-6cb2d9a9e6cd"
      },
      "source": [
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/libs/libs-kafka_301.zip --directory-prefix=/content/spark-3.0.1-bin-hadoop2.7/jars/\n",
        "!unzip -n /content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip -d /content/spark-3.0.1-bin-hadoop2.7/jars/\n",
        "!ls /content/spark-3.0.1-bin-hadoop2.7/jars/*kafka*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip\n",
            "  inflating: /content/spark-3.0.1-bin-hadoop2.7/jars/ant-1.10.3.jar  \n",
            "  inflating: /content/spark-3.0.1-bin-hadoop2.7/jars/ant-launcher-1.10.3.jar  \n",
            "  inflating: /content/spark-3.0.1-bin-hadoop2.7/jars/asm-7.0.jar  \n",
            "  inflating: /content/spark-3.0.1-bin-hadoop2.7/jars/asm-tree-7.0.jar  \n",
            "  inflating: /content/spark-3.0.1-bin-hadoop2.7/jars/asm-util-7.0.jar  \n",
            "  inflating: /content/spark-3.0.1-bin-hadoop2.7/jars/cglib-3.2.10.jar  \n",
            "  inflating: /content/spark-3.0.1-bin-hadoop2.7/jars/commons-pool2-2.6.2.jar  \n",
            "  inflating: /content/spark-3.0.1-bin-hadoop2.7/jars/kafka-clients-2.4.1.jar  \n",
            "  inflating: /content/spark-3.0.1-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.12-3.0.1.jar  \n",
            "  inflating: /content/spark-3.0.1-bin-hadoop2.7/jars/spark-token-provider-kafka-0-10_2.12-3.0.1.jar  \n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/kafka-clients-2.4.1.jar\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.12-3.0.1.jar\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/spark-token-provider-kafka-0-10_2.12-3.0.1.jar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0qDLAxMTUYQ"
      },
      "source": [
        "Define the environment (Java & Spark homes)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSd4dfANNndH"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_IgZEv2XaDm"
      },
      "source": [
        "Starting Spark Session and print the version\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLMbVBATf9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "2a1e7f2f-45f5-4e67-cefe-d5df77b1c00b"
      },
      "source": [
        "import findspark\n",
        "findspark.add_packages([\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1\"])\n",
        "findspark.add_jars([\"/content/spark-3.0.1-bin-hadoop2.7/jars/kafka-clients-2.0.0.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/lz4-java-1.4.1-jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/scala-library-2.11.12.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/slf4j-api-1.7.25.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.11-2.4.5.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/spark-tags_2.11-2.4.5.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/unused-1.0.0.jar\"])\n",
        "findspark.init(\"spark-3.0.1-bin-hadoop2.7\")# SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.ui.port\", \"4050\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.0.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G28MgeRJHKJ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "67784ef0-05df-4976-dbe4-9accf75686bd"
      },
      "source": [
        "spark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://267c51ed52f1:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f38740b6b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrOO29YRuDB"
      },
      "source": [
        "# For Pandas conversion optimization\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNiYuI5dGo8Y"
      },
      "source": [
        "Creating ngrok tunnel to allow Spark UI (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-7fXZiGmqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4edbdf14-1890-4cf5-e94f-250c5ef4cf2f"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-30 09:20:58--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.200.34.95, 3.226.107.193, 52.206.211.104, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.200.34.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  36.8MB/s    in 0.4s    \n",
            "\n",
            "2021-01-30 09:20:58 (36.8 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruHocwYcT4aj"
      },
      "source": [
        "# Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musWXLzBUEQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def3fcfa-e6a7-4c03-b62b-67a02cfad6c2"
      },
      "source": [
        "!mkdir -p /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/trades.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/trades.json -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.edges.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.address.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.intermediary.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.officer.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.entity.csv -P /dataset\r\n",
        "!ls /dataset"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "offshore_leaks.edges.csv\t       offshore_leaks.nodes.officer.csv\n",
            "offshore_leaks.nodes.address.csv       trades.csv\n",
            "offshore_leaks.nodes.entity.csv        trades.json\n",
            "offshore_leaks.nodes.intermediary.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42PI1onm9kIh"
      },
      "source": [
        "# Project 1 - Regulatory Banking Project\r\n",
        "---\r\n",
        "\r\n",
        "Input files: /dataset/trades.csv & /dataset/trades.json\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebjNK66MgA_i"
      },
      "source": [
        "df_trade1 = spark \\\n",
        ".read \\\n",
        ".format(\"csv\") \\\n",
        ".option(\"inferSchema\", \"true\") \\\n",
        ".option(\"header\", \"true\") \\\n",
        ".load(\"/dataset/trades.csv\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt8_v7u7vG9K",
        "outputId": "416f5501-f334-44f1-e6b0-bb55c98c05dc"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "def pnl(open, close):\n",
        "  if (open is not None and close is not None):\n",
        "    return (close * 100 / open) - 100\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "spark.udf.register(\"pnl_udf\", pnl)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.pnl>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lti0wXCJw6EW",
        "outputId": "60a6779d-b97c-4f43-e7f1-b319e369936c"
      },
      "source": [
        "df_trade1_pnl = (df_trade1\n",
        ".select(\"Date\", \"Open\", \"Close\")\n",
        ".withColumn(\"pnl\", round(pnl(col(\"open\"), col(\"close\")), 2))\n",
        ".withColumn(\"trade\", lit(\"trade_1\"))\n",
        ")\n",
        "\n",
        "df_trade1_pnl.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+------+-----+-------+\n",
            "|      Date|   Open| Close|  pnl|  trade|\n",
            "+----------+-------+------+-----+-------+\n",
            "|2017-12-28| 190.91|189.78|-0.59|trade_1|\n",
            "|2017-12-27|  190.6|190.19|-0.22|trade_1|\n",
            "|2017-12-26| 188.53|190.36| 0.97|trade_1|\n",
            "|2017-12-22|  188.2|188.13|-0.04|trade_1|\n",
            "|2017-12-21|  187.7|188.08|  0.2|trade_1|\n",
            "|2017-12-20| 187.14|187.31| 0.09|trade_1|\n",
            "|2017-12-19|185.235|185.98|  0.4|trade_1|\n",
            "|2017-12-18| 183.96|184.73| 0.42|trade_1|\n",
            "|2017-12-15|183.005|182.58|-0.23|trade_1|\n",
            "|2017-12-14| 183.88|182.13|-0.95|trade_1|\n",
            "|2017-12-13| 182.01|183.03| 0.56|trade_1|\n",
            "|2017-12-12| 182.75| 181.8|-0.52|trade_1|\n",
            "|2017-12-11|  182.9|182.25|-0.36|trade_1|\n",
            "|2017-12-08|  182.5|183.41|  0.5|trade_1|\n",
            "|2017-12-07| 180.05| 182.0| 1.08|trade_1|\n",
            "|2017-12-06| 180.25| 180.8| 0.31|trade_1|\n",
            "|2017-12-05| 184.79|182.85|-1.05|trade_1|\n",
            "|2017-12-04| 183.19| 184.9| 0.93|trade_1|\n",
            "|2017-12-01| 180.32|180.42| 0.06|trade_1|\n",
            "|2017-11-30| 178.07|179.82| 0.98|trade_1|\n",
            "+----------+-------+------+-----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eohBjdVToVMR"
      },
      "source": [
        "df_trade2 = spark \\\n",
        ".read \\\n",
        ".format(\"json\") \\\n",
        ".option(\"inferSchema\", \"true\") \\\n",
        ".option(\"header\", \"true\") \\\n",
        ".load(\"/dataset/trades.json\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inT8gagjoryY"
      },
      "source": [
        "df_trade2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h6LICd9ZBPw"
      },
      "source": [
        "# Project 2 - Transactions Notifications\r\n",
        "\r\n",
        "*Hint: https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49tIaJ7gZN2C"
      },
      "source": [
        "from pyspark.sql.functions import from_json, col\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType\r\n",
        "\r\n",
        "df = spark \\\r\n",
        "  .readStream \\\r\n",
        "  .format(\"kafka\") \\\r\n",
        "  .option(\"kafka.bootstrap.servers\", \"ec2-34-232-50-77.compute-1.amazonaws.com:9092\") \\\r\n",
        "  .option(\"subscribe\", \"transactions\") \\\r\n",
        "  .load()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6SLWzYRZbud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db23d22-73df-4ef9-9fca-f4e1d7b6a3bf"
      },
      "source": [
        "schema = StructType(\r\n",
        "    [\r\n",
        "     StructField('Account No', StringType(), True),\r\n",
        "     StructField('DATE', StringType(), True),\r\n",
        "     StructField('TRANSACTION DETAILS', StringType(), True),\r\n",
        "     StructField('CHQ.NO.', StringType(), True),\r\n",
        "     StructField('VALUE DATE', StringType(), True),\r\n",
        "     StructField(' WITHDRAWAL AMT ', StringType(), True),\r\n",
        "     StructField(' DEPOSIT AMT ', StringType(), True),\r\n",
        "     StructField('BALANCE AMT', StringType(), True)\r\n",
        "    ]\r\n",
        ")\r\n",
        "df.printSchema()\r\n",
        "\r\n",
        "dataset = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\") \\\r\n",
        "    .withColumn(\"value\", from_json(\"value\", schema)) \\\r\n",
        "    .select(col('key'), col(\"timestamp\"), col('value.*'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh82Gqz7Z1bm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a8c95c-3791-431c-ced2-5b600f309dc7"
      },
      "source": [
        "dataset.writeStream \\\r\n",
        " .outputMode(\"update\") \\\r\n",
        " .format(\"memory\") \\\r\n",
        " .option(\"truncate\", \"false\") \\\r\n",
        " .queryName(\"transactions\") \\\r\n",
        " .start()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.StreamingQuery at 0x7f3872f7b860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2KkvViQaXKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41cb0759-efa0-4d26-9fa0-949d7e9992e9"
      },
      "source": [
        "spark.sql(\"select * from transactions\").show(truncate = False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----------------------+-------------+---------+-------------------------+-------+----------+----------------+-------------+-----------------+\n",
            "|key |timestamp              |Account No   |DATE     |TRANSACTION DETAILS      |CHQ.NO.|VALUE DATE| WITHDRAWAL AMT | DEPOSIT AMT |BALANCE AMT      |\n",
            "+----+-----------------------+-------------+---------+-------------------------+-------+----------+----------------+-------------+-----------------+\n",
            "|null|2021-01-30 12:05:12.259|409000438611'|23-ago-16|RUPAY POS ACQ SETTL 23081|       |23-ago-16 |                | 1.778,24    | -547.706.347,07 |\n",
            "|null|2021-01-30 12:05:12.256|409000438611'|20-ago-16|RUPAY POS ACQ SETTL 20081|       |20-ago-16 |                | 5.047,05    | -547.721.273,80 |\n",
            "|null|2021-01-30 12:05:12.253|409000438611'|18-ago-16|RUPAY POS ACQ SETTL 18081|       |18-ago-16 |                | 13.225,86   | -547.730.739,40 |\n",
            "|null|2021-01-30 12:05:12.257|409000438611'|22-ago-16|RUPAY POS ACQ SETTL 21081|       |22-ago-16 |                | 13.148,49   | -547.708.125,31 |\n",
            "|null|2021-01-30 12:05:12.261|409000438611'|24-ago-16|RUPAY POS ACQ SETTL 24081|       |24-ago-16 |                | 3.357,09    | -547.702.989,98 |\n",
            "|null|2021-01-30 12:05:12.251|409000438611'|18-ago-16|RUPAY POS ACQ SETTL 17081|       |18-ago-16 |                | 9.350,56    | -547.743.965,26 |\n",
            "|null|2021-01-30 12:05:12.254|409000438611'|19-ago-16|RUPAY POS ACQ SETTL 19081|       |19-ago-16 |                | 4.418,55    | -547.726.320,85 |\n",
            "|null|2021-01-30 12:05:12.262|409000438611'|25-ago-16|RUPAY POS ACQ SETTL 25081|       |25-ago-16 |                | 9.693,39    | -547.693.296,59 |\n",
            "|null|2021-01-30 12:05:12.265|409000438611'|29-ago-16|RUPAY POS ACQ SETTL 27081|       |29-ago-16 |                | 6.616,78    | -547.679.519,97 |\n",
            "|null|2021-01-30 12:05:12.263|409000438611'|26-ago-16|RUPAY POS ACQ SETTL 26081|       |26-ago-16 |                | 7.159,84    | -547.686.136,75 |\n",
            "|null|2021-01-30 12:05:12.276|409000438611'|03-sep-16|RUPAY POS ACQ SETTL 03091|       |03-sep-16 |                | 1.428,96    | -547.603.987,86 |\n",
            "|null|2021-01-30 12:05:12.28 |409000438611'|08-sep-16|RUPAY POS ACQ SETTL 08091|       |08-sep-16 |                | 3.165,36    | -547.596.314,35 |\n",
            "|null|2021-01-30 12:05:12.287|409000438611'|14-sep-16|RUPAY POS ACQ SETTL 13091|       |14-sep-16 |                | 60.193,54   | -547.510.894,27 |\n",
            "|null|2021-01-30 12:05:12.292|409000438611'|16-sep-16|RUPAY POS ACQ SETTL 16091|       |16-sep-16 |                | 19.788,26   | -547.462.740,03 |\n",
            "|null|2021-01-30 12:05:12.283|409000438611'|12-sep-16|RUPAY POS ACQ SETTL 10091|       |12-sep-16 |                | 9.961,95    | -547.582.476,20 |\n",
            "|null|2021-01-30 12:05:12.317|409000438611'|01-oct-16|RUPAY POS ACQ SETTL 01101|       |01-oct-16 |                | 10.923,77   | -547.528.613,23 |\n",
            "|null|2021-01-30 12:05:12.315|409000438611'|30-sep-16|RUPAY POS ACQ SETTL 30091|       |30-sep-16 |                | 5.765,13    | -547.539.537,00 |\n",
            "|null|2021-01-30 12:05:12.285|409000438611'|12-sep-16|RUPAY POS ACQ SETTL 11091|       |12-sep-16 |                | 9.668,34    | -547.572.807,86 |\n",
            "|null|2021-01-30 12:05:12.296|409000438611'|19-sep-16|RUPAY POS ACQ SETTL 19091|       |19-sep-16 |                | 19.857,93   | -547.431.237,28 |\n",
            "|null|2021-01-30 12:05:12.304|409000438611'|22-sep-16|RUPAY POS ACQ SETTL 22091|       |22-sep-16 |                | 14.875,22   | -547.595.744,39 |\n",
            "+----+-----------------------+-------------+---------+-------------------------+-------+----------+----------------+-------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjDnpWADMSG2"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "dataset2 = \\\n",
        "  dataset \\\n",
        "    .groupBy(window(\"timestamp\", \"5 seconds\")) \\\n",
        "    .count()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUH3ZBxFPzAH",
        "outputId": "3ae56537-93aa-4f8d-9264-3b2feefec948"
      },
      "source": [
        "dataset2.writeStream \\\n",
        " .outputMode(\"update\") \\\n",
        " .format(\"memory\") \\\n",
        " .option(\"truncate\", \"false\") \\\n",
        " .queryName(\"transactions2\") \\\n",
        " .start()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.StreamingQuery at 0x7f3872fb8ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN5LzCHWP-j6",
        "outputId": "f302f717-0542-48e7-fe65-be695467e7b7"
      },
      "source": [
        "spark.sql(\"select * from transactions2\").show(truncate = False)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------+-----+\n",
            "|window                                    |count|\n",
            "+------------------------------------------+-----+\n",
            "|[2021-01-30 12:25:35, 2021-01-30 12:25:40]|50   |\n",
            "|[2021-01-30 12:25:40, 2021-01-30 12:25:45]|50   |\n",
            "|[2021-01-30 12:25:50, 2021-01-30 12:25:55]|50   |\n",
            "|[2021-01-30 12:25:45, 2021-01-30 12:25:50]|50   |\n",
            "|[2021-01-30 12:25:55, 2021-01-30 12:26:00]|50   |\n",
            "+------------------------------------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG8_q4f5Q_E6"
      },
      "source": [
        "# Project 3 - Panama Papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NApu7049VPwm"
      },
      "source": [
        "Trace \"Spring Song International Co., Ltd.\" entity with Spark SQL using the following dataset</br>\r\n",
        "/dataset/offshore_leaks.nodes.entity.csv </br>\r\n",
        "/dataset/offshore_leaks.nodes.intermediary.csv </br>\r\n",
        "/dataset/offshore_leaks.edges.csv </br>\r\n",
        "/dataset/offshore_leaks.nodes.officer.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ydUZNSrYGyN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}